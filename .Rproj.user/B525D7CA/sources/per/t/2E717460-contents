---
title: "assignment 7"
author: "Yong Qiao"
date: "10/24/2020"
output: pdf_document
---
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\bx}{\textbf{x}}
\newcommand{\bX}{\textbf{X}}
\newcommand{\bz}{\textbf{z}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## 5.3.1

Let $f$ and $g$ be two probability densities on $(0,\infty)$, such that
\begin{align*}
  f(x) \propto \sqrt{4+x}\,x^{\theta-1} e^{-x}, \quad
  g(x) \propto (2 x^{\theta-1} + x^{\theta-1/2}) e^{-x}, \quad x>0.
\end{align*}

- Find the value of the normalizing constant for $g$, i.e.,
  the constant $C$ such that
  \begin{align*}
    C\int_0^\infty (2 x^{\theta-1} +  x^{\theta-1/2}) e^{-x} \dd x=1.
  \end{align*}
  Show that $g$ is a mixture of Gamma distributions.  Identify the
  component distributions and their weights in the mixture.
- Design a procedure (pseudo-code) to sample from $g$;
  implement it in an R function;
  draw a sample of size $n = 10,000$ using your function for at least one
  $\theta$ value; 
  plot the kernel density estimation of $g$ from your sample and the true
  density in one figure. 
- Design a procedure (pseudo-code) to use rejection sampling to sample from
  $f$ using $g$ as the instrumental distribution.  Overlay the estimated
  kernel density of a random sample generated by your procedure and $f$.

## Solution 
$$\begin{aligned}
C \int_0^\infty(2x^{\theta-1}+x^{\theta-0.5})e^{-x}dx
& = 2C \int_0^\infty x^{\theta-1}e^{-x}dx+C \int_0^\infty x^{\theta-0.5}e^{-x}dx \\
& = 2C Gamma(\theta,1)+C Gamma(\theta+0.5,1) \\
& = C 2\tau(\theta)+\tau(\theta+0.5) \\
& = 1
\end{aligned}$$
so we can get the following
$$C =\frac{1}{2\tau(\theta)+\tau(\theta+0.5)}$$
$$g(x)=\frac{2}{2\tau(\theta)+\tau(\theta+0.5)} Gamma(\theta,1) + \frac{1}{2\tau(\theta)+\tau(\theta+0.5)} Gamma(\theta +0.5,1)$$
so the distribution is $Gamma(\theta,1)$  with weight $\frac{2}{3}$ and $Gamma(\theta,1)$  with weight $\frac{2}{3}$$Gamma(\theta+0.5,1)$  with weight $\frac{1}{3}$

```{r}
for (theta in 1:5) {
  c <- 1 / (2 * gamma(theta) + gamma(theta + 0.5))
  w1 <- c * 2 / (c * 2 + c); w2 <- c / (c * 2 + c)
  s1 <- theta
  s2 <- theta + 0.5
  n <- 10000
  set.seed(123)
  u <- rbinom(n, prob = w1, size = 1)
  x <- rgamma(n, shape = ifelse(u == 1, s1, s2), scale = 1)
  g <- w1 * dgamma(x, shape = s1, scale = 1) +w2 * dgamma(x, shape = s2, scale = 1)
  g.true <- c * exp(-x) * (2 * x^(theta - 1) + x^(theta - 0.5))
  plot(x,g,main =paste("theta value is", theta))
  points(x,g.true,col="red")
}

```
$$\alpha = sup \frac{q(x)}{g(x)} = sup \frac{\sqrt{x+4}}{2+\sqrt{x}}=1$$
```{r}
x.f <- rep(NA, n)
for (i in 1:n) {
  while (TRUE) {
    u <- rbinom(1, prob = w1, size = 1)
    cand <- rgamma(1, shape = ifelse(u == 1, s1, s2), scale = 1)
    ratio <- sqrt(cand + 4)/(2+sqrt(cand))  
    uni <- runif(1)
    if (uni < ratio) break
  }
  x.f[i] <- cand
}

q <- sqrt(x.f + 4) * x.f^(theta - 1) * exp(-x.f)
plot(density(x.f))
points(x.f,q/70,col="red")
```
we can see that the tow plot are similar to each other 


## 6.3.1

Consider again the normal mixture example, except that the parameters of the normal distributions are considered unknown. Suppose that prior for $\mu_1$ and $\mu_2$ are $N(0, 10^2)$, that the prior for $1/\sigma_1^2$ and $1/\sigma_2^2$ are $\Gamma(a, b)$ with shape $a = .5$ and scale $b = 10$. Further, all the priors are independent. Design an MCMC using the Gibbs sampling approach to estimate all 5 parameters. You may use the `arms()`function in package **HI**. Run your chain for sufficiently long and drop the burn-in period. Plot the histogram of the results for all the parameters.
```{r,warning=FALSE}
library(MCMCpack)
delta <- 0.7
n <- 100
set.seed(123)
u <- rbinom(n, prob = delta, size = 1)
x <- rnorm(n, ifelse(u == 1, 7, 10), 1)
logpost <- function(theta, x) {
  delta <- theta[1]
  mu1 <- theta[2]; mu2 <- theta[3]
  sig1 <- theta[4]; sig2 <- theta[5]
  re <- sum(log(delta * dnorm(x, mu1,sqrt(sig1)) +(1-delta)*dnorm(x,mu2,sqrt(sig2))))+log(dnorm(mu1,0,10) ) + log(dnorm(mu2,0,10) ) + log(dinvgamma(sig1, 0.5 , 0.1) ) +  log(dinvgamma(sig2, 0.5 , 0.1) )
  return(re)
}
mymcmc <- function(niter, thetaInit, x, nburn= 100) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, x)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  for (i in 2:niter) {
    for (j in 1:p) {
      out[i, j] <- thetaCurrent[j] <- if (j < 2) {
        HI::arms(thetaCurrent[j], logFC,
                 function(x, idx) ((x > 0) * (x < 1)), 
                 1, idx = j)
      } else if (j < 4 ) { 
        HI::arms(thetaCurrent[j], logFC,
                 function(x, idx) ((x > -30) * (x < 30)), 
                 1, idx = j)
      } else {
        HI::arms(thetaCurrent[j], logFC,
                 function(x, idx) ((x > 0) * (x < 100)), 
                 1, idx = j)
      }
    }
  }
  out[-(1:nburn), ]
}

niter <- 6000; nburn <- 1000
thetaInit <- c(0.5, 2, 2, 2, 2)
sim <- mymcmc(niter, thetaInit, x)

hist((sim[,1]), main = 'Delta',xlab = 'Delta')
hist((sim[,2]), main = 'mu1',xlab = 'mu1')
hist((sim[,3]), main = 'mu2',xlab = 'mu2')
hist((sim[,4]), main = 'sigma1 square',xlab = 'sigma1 square')
hist((sim[,5]), main = 'sigma2 square',xlab = 'sigma2 square')
```

